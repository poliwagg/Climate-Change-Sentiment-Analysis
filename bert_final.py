# -*- coding: utf-8 -*-
"""bertmodel_randomsearch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1of7r9qI73DTEQgm9B4CZL4e6NuMWZbf8
"""

import torch
from torch import nn
from torch.optim import Adam
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix, classification_report, f1_score
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import copy
import pickle

def load_dataset(data_file):
  df = pd.read_csv(data_file)
  texts = df['message'].tolist()
  labels = df['sentiment'].tolist()
  return texts, labels

class TextClassificationDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        text = str(self.texts[idx]) 
        label = self.labels[idx]
        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)
        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)} 

class BERTClassifier(nn.Module):
    def __init__(self, bert_model_name, num_classes, dropout):
        super(BERTClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        x = self.dropout(pooled_output)
        logits = self.fc(x)
        return logits


def train(model, df, learning_rate, epochs, batch_size=16, num_folds=5, max_length=256):
    

    bert_model_name = 'bert-base-uncased'
    tokenizer = BertTokenizer.from_pretrained(bert_model_name)
    
    history = {}
    kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=123)
    X = df['message'].values  
    y = df['sentiment'].values
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    best_model = None
    best_val_f1 = 0
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y), 1):
        print(f"\n--- Fold {fold} ---")
        history[fold] = {"train_loss": [], "train_acc": [], "train_f1": [], 
                        "val_loss": [], "val_acc": [], "val_f1": []}
        
        
        train_texts = X[train_idx].tolist()
        train_labels = y[train_idx].tolist()
        val_texts = X[val_idx].tolist()
        val_labels = y[val_idx].tolist()
        
        
        train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)
        val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)
        
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        
        fold_model = copy.deepcopy(model)
        fold_model.to(device)
        
        
        criterion = nn.CrossEntropyLoss()
        optimizer = Adam(fold_model.parameters(), lr=learning_rate)
        
        
        for epoch in range(epochs):
            
            fold_model.train()
            train_loss = 0.0
            train_correct = 0
            total_train_samples = 0
            train_preds = []
            train_labels_epoch = []
            
            for batch in tqdm(train_dataloader, desc=f"Fold {fold} Epoch {epoch+1}/{epochs}"):
                optimizer.zero_grad()
                
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels_batch = batch['label'].to(device)
                
                outputs = fold_model(input_ids, attention_mask)
                loss = criterion(outputs, labels_batch)
                
                loss.backward()
                optimizer.step()
                
                
                batch_size_current = input_ids.size(0)
                train_loss += loss.item() * batch_size_current
                preds = outputs.argmax(dim=1)
                train_correct += (preds == labels_batch).sum().item()
                total_train_samples += batch_size_current
                
                
                train_preds.extend(preds.cpu().tolist())
                train_labels_epoch.extend(labels_batch.cpu().tolist())
            
            
            avg_train_loss = train_loss / total_train_samples
            train_accuracy = train_correct / total_train_samples
            train_f1 = f1_score(train_labels_epoch, train_preds, average='weighted') # telah menggunakan sklearn utk metriknya
            
            
            fold_model.eval()
            val_loss = 0.0
            val_correct = 0
            total_val_samples = 0
            val_preds = []
            val_labels_epoch = []
            
            with torch.no_grad():
                for batch in val_dataloader:
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    labels_batch = batch['label'].to(device)
                    
                    outputs = fold_model(input_ids, attention_mask)
                    loss = criterion(outputs, labels_batch)
                    
                    
                    batch_size_current = input_ids.size(0)
                    val_loss += loss.item() * batch_size_current
                    preds = outputs.argmax(dim=1)
                    val_correct += (preds == labels_batch).sum().item()
                    total_val_samples += batch_size_current
                    
                    
                    val_preds.extend(preds.cpu().tolist())
                    val_labels_epoch.extend(labels_batch.cpu().tolist())
            
            
            avg_val_loss = val_loss / total_val_samples
            val_accuracy = val_correct / total_val_samples
            val_f1 = f1_score(val_labels_epoch, val_preds, average='weighted') # telah menggunakan sklearn utk metriknya
            
           
            history[fold]["train_loss"].append(avg_train_loss)
            history[fold]["train_acc"].append(train_accuracy)
            history[fold]["train_f1"].append(train_f1)
            history[fold]["val_loss"].append(avg_val_loss)
            history[fold]["val_acc"].append(val_accuracy)
            history[fold]["val_f1"].append(val_f1)
            
            
            print(f"Fold {fold}, Epoch {epoch+1}/{epochs}: "
                  f"Train Loss: {avg_train_loss:.3f}, Train Acc: {train_accuracy:.3f}, Train F1: {train_f1:.3f} | "
                  f"Val Loss: {avg_val_loss:.3f}, Val Acc: {val_accuracy:.3f}, Val F1: {val_f1:.3f}")
        
        
        fold_val_f1 = max(history[fold]["val_f1"])
        if fold_val_f1 > best_val_f1:
            best_val_f1 = fold_val_f1
            best_model = copy.deepcopy(fold_model)
            print(f"New best model found in Fold {fold} with Val F1: {best_val_f1:.4f}")
    
    return history, best_model, best_val_f1

def evaluate(model, df_test, batch_size=16):
    
    test_dataset = TextClassificationDataset(df_test)
    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    total_correct = 0
    total_samples = 0
    all_preds = []
    all_labels = []

    model.eval()
    with torch.no_grad():
        for batch in test_dataloader:
            input_ids = batch['input_ids'].to(device).squeeze(1)
            attention_mask = batch['attention_mask'].to(device)
            labels_batch = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask)
            preds = outputs.argmax(dim=1)

            total_correct += (preds == labels_batch).sum().item()
            total_samples += input_ids.size(0)

            all_preds.extend(preds.cpu().tolist())
            all_labels.extend(labels_batch.cpu().tolist())

    accuracy = total_correct / total_samples
    f1 = f1_score(all_labels, all_preds, average='weighted')
    
    target_names = ['0', '1', '2']
    class_report = classification_report(all_labels, all_preds, target_names=target_names, digits=4)
    conf_matrix = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2])

    print(f"Test Accuracy: {accuracy:.3f}")
    print(f"Test Weighted F1 Score: {f1:.3f}")
    print("---------------------------------------------------------------------")
    print(f"Test Data:\n{class_report}")
    print(f"COnfusion Matrix:\n{conf_matrix}")
    
    return f1, class_report, conf_matrix

df_train = pd.read_csv("df_train.csv")
df_train = df_train.dropna(subset=['message'])
df_test = pd.read_csv("df_test.csv")

label_mapping = {-1: 0, 0: 1, 1: 2}
df_train['sentiment'] = df_train['sentiment'].map(label_mapping)
df_test['sentiment'] = df_test['sentiment'].map(label_mapping)

bert_model_name = 'bert-base-uncased'
num_classes = 3
dropout = 0.3

model = BERTClassifier(bert_model_name, num_classes, dropout)

learning_rate = 5e-5
epochs = 10

history, best_model, best_val_f1 = train(model, df_train, learning_rate=learning_rate, epochs=epochs, batch_size=32)

with open("history_train_bert_final.pkl", "wb") as f:
    pickle.dump(history, f)

torch.save(best_model.state_dict(), "best_model_bert_final.pt")


## Random Search
# data_file = "df_train.csv"
# df = pd.read_csv(data_file)
# df = df.dropna(subset=['message'])
    

# label_mapping = {-1: 0, 0: 1, 1: 2}
# df['sentiment'] = df['sentiment'].map(label_mapping)
    
    
# bert_model_name = 'bert-base-uncased'
# learning_rates = [1e-4, 1e-5, 3e-5, 5e-5, 5e-6]
# dropouts = [0.1, 0.2, 0.3, 0.4, 0.5]
# batch_sizes = [16, 32, 64]
# epochs_list = [10]
# num_folds_list = [2]
# num_trials = 30  


# trial_results = []  
# best_score = 0  
# best_params = None
# best_model_overall = None


# for trial in range(num_trials):
    
#     lr = random.choice(learning_rates)
#     dropout = random.choice(dropouts)
#     batch_size = random.choice(batch_sizes)
#     epochs = random.choice(epochs_list)
#     num_folds = random.choice(num_folds_list)
    
#     print(f"\n==== Trial {trial+1}/{num_trials} ====")
#     print(f"Parameters: lr={lr}, dropout={dropout}, batch_size={batch_size}, epochs={epochs}, num_folds={num_folds}")
    
    
#     model = BERTClassifier(bert_model_name, 3, dropout) 
    
    
#     history, best_val_f1 = train_with_kfold(
#         model=model,
#         df=df,
#         learning_rate=lr,
#         batch_size=batch_size,
#         epochs=epochs,
#         num_folds=num_folds
#     )
    
#     print(f"Validation F1 Terbaik ={best_val_f1}")
    
#  
#     trial_result = {
#         'trial': trial + 1,
#         'learning_rate': lr,
#         'dropout': dropout,
#         'batch_size': batch_size,
#         'epochs': epochs,
#         'num_folds': num_folds,
#         'avg_val_f1': best_val_f1,
#         # 'test_f1': test_f1
#     }
#     trial_results.append(trial_result)
    
    
#     if best_val_f1 > best_score:
#         best_score = best_val_f1
#         best_params = trial_result
#         # best_model_overall = copy.deepcopy(best_trial_model)

# print("\n========== Random Search Results ==========")
# print("Best Hyperparameters:", best_params)
# print(f"Best Test F1 Score: {best_score:.4f}")

# df_results = pd.DataFrame(trial_results)
# df_results.to_csv("bertmodel_trial_results.csv", index=False)



