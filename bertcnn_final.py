# -*- coding: utf-8 -*-
"""model newest (f1 version).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_KH_QwQfxS6eIdkhBPVGDqEnRW4Wj7VM
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from transformers import BertTokenizer, AutoModel
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from torch.utils.data import  Dataset
from sklearn.model_selection import train_test_split, StratifiedKFold
from torch.optim import Adam
from tqdm import tqdm
from sklearn.metrics import classification_report
import random
import torch.optim as optim
import copy
import pickle

df_train= pd.read_csv('df_train.csv')
df_train = df_train.dropna(subset=['message'])
df_test= pd.read_csv('df_test.csv')

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

labels = {-1:0, 0:1, 1:2}

class Dataset(torch.utils.data.Dataset):

    def __init__(self, df):
        df['message'] = df['message'].astype(str)
        self.labels = [labels[label] for label in df['sentiment']]
        self.encodings = tokenizer(
            list(df['message']),
            padding='max_length',
            max_length=256,
            truncation=True,
            return_tensors="pt"
        )

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

class BertCNN(nn.Module):
    def __init__(self, hidden_size=768, num_channels=256, kernel_size=7, dropout=0.2):
        super(BertCNN, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv1d(in_channels=hidden_size, out_channels=num_channels, kernel_size=kernel_size, padding=0),
            nn.ReLU(),
            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=5, padding=0),
            nn.ReLU(),
            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=5, padding=0),
            nn.ReLU(),

        )
        self.pool = nn.AdaptiveMaxPool1d(1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = self.conv_layers(x)
        x = self.pool(x)
        x = x.squeeze(-1)
        x = self.dropout(x)
        return x

class Classification(nn.Module):
    def __init__(self, input_dim=128, hidden_dim=128, num_classes=3, dropout=0.2):
        super(Classification, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()

        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)

        x = self.fc2(x)
        x = self.relu(x)
        x = self.dropout(x)

        x = self.fc3(x)
        return x

class Model(nn.Module):
    def __init__(self, dropout=0.2):
        super(Model, self).__init__()
        self.bert = AutoModel.from_pretrained('bert-base-uncased')
        hidden_size = self.bert.config.hidden_size
        self.bert_cnn = BertCNN(hidden_size=hidden_size, num_channels=256, kernel_size=7, dropout=dropout)
        self.classifier = Classification(input_dim=128, hidden_dim=128, num_classes=3, dropout=dropout)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)
        sequence_output = outputs[0]
        cnn_features = self.bert_cnn(sequence_output)
        logits = self.classifier(cnn_features)
        return logits

def train(model, df, learning_rate, epochs, batch_size=16, num_folds=5):

    history = {}
    kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=123)
    X = df['message'].values
    y = df['sentiment'].values

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    best_model = None
    best_val_f1 = 0

    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y), 1):
        print(f"\n--- Fold {fold} ---")
        history[fold] = {"train_loss": [], "train_acc": [], "train_f1": [],
                        "val_loss": [], "val_acc": [], "val_f1": []}

        train_df = df.iloc[train_idx].reset_index(drop=True)
        val_df = df.iloc[val_idx].reset_index(drop=True)

        train_dataset = Dataset(train_df)
        val_dataset = Dataset(val_df)

        train_dataloader = torch.utils.data.DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True
        )
        val_dataloader = torch.utils.data.DataLoader(
            val_dataset, batch_size=batch_size, shuffle=False
        )

        fold_model = copy.deepcopy(model)
        fold_model.to(device)

        criterion = nn.CrossEntropyLoss()
        optimizer = Adam(fold_model.parameters(), lr=learning_rate)

        for epoch in range(epochs):
            fold_model.train()
            train_loss = 0.0
            train_correct = 0
            total_train_samples = 0
            train_preds = []
            train_labels = []

            for batch in tqdm(train_dataloader, desc=f"Fold {fold} Epoch {epoch+1}/{epochs}"):
                optimizer.zero_grad()

                input_ids = batch['input_ids'].to(device).squeeze(1)
                attention_mask = batch['attention_mask'].to(device)
                labels_batch = batch['labels'].to(device)

                outputs = fold_model(input_ids, attention_mask)
                loss = criterion(outputs, labels_batch)

                loss.backward()
                optimizer.step()

                batch_size_current = input_ids.size(0)
                train_loss += loss.item() * batch_size_current
                preds = outputs.argmax(dim=1)
                train_correct += (preds == labels_batch).sum().item()
                total_train_samples += batch_size_current

                train_preds.extend(preds.cpu().tolist())
                train_labels.extend(labels_batch.cpu().tolist())

            avg_train_loss = train_loss / total_train_samples
            train_accuracy = train_correct / total_train_samples
            train_f1 = f1_score(train_labels, train_preds, average='weighted')

            fold_model.eval()
            val_loss = 0.0
            val_correct = 0
            total_val_samples = 0
            val_preds = []
            val_labels = []

            with torch.no_grad():
                for batch in val_dataloader:
                    input_ids = batch['input_ids'].to(device).squeeze(1)
                    attention_mask = batch['attention_mask'].to(device)
                    labels_batch = batch['labels'].to(device)

                    outputs = fold_model(input_ids, attention_mask)
                    loss = criterion(outputs, labels_batch)

                    batch_size_current = input_ids.size(0)
                    val_loss += loss.item() * batch_size_current
                    preds = outputs.argmax(dim=1)
                    val_correct += (preds == labels_batch).sum().item()
                    total_val_samples += batch_size_current
                    val_preds.extend(preds.cpu().tolist())
                    val_labels.extend(labels_batch.cpu().tolist())

            avg_val_loss = val_loss / total_val_samples
            val_accuracy = val_correct / total_val_samples
            val_f1 = f1_score(val_labels, val_preds, average='weighted')

            history[fold]["train_loss"].append(avg_train_loss)
            history[fold]["train_acc"].append(train_accuracy)
            history[fold]["train_f1"].append(train_f1)
            history[fold]["val_loss"].append(avg_val_loss)
            history[fold]["val_acc"].append(val_accuracy)
            history[fold]["val_f1"].append(val_f1)

            print(f"Fold {fold}, Epoch {epoch+1}/{epochs}: "
                  f"Train Loss: {avg_train_loss:.3f}, Train Acc: {train_accuracy:.3f}, Train F1: {train_f1:.3f} | "
                  f"Val Loss: {avg_val_loss:.3f}, Val Acc: {val_accuracy:.3f}, Val F1: {val_f1:.3f}")

        fold_val_f1 = max(history[fold]["val_f1"])
        if fold_val_f1 > best_val_f1:
            best_val_f1 = fold_val_f1
            best_model = copy.deepcopy(fold_model)

    return history, best_model, best_val_f1

def evaluate(model, df_test, batch_size=16):

    test_dataset = Dataset(df_test)
    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    total_correct = 0
    total_samples = 0
    all_preds = []
    all_labels = []

    model.eval()
    with torch.no_grad():
        for batch in test_dataloader:
            input_ids = batch['input_ids'].to(device).squeeze(1)
            attention_mask = batch['attention_mask'].to(device)
            labels_batch = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask)
            preds = outputs.argmax(dim=1)

            total_correct += (preds == labels_batch).sum().item()
            total_samples += input_ids.size(0)

            all_preds.extend(preds.cpu().tolist())
            all_labels.extend(labels_batch.cpu().tolist())

    accuracy = total_correct / total_samples
    f1 = f1_score(all_labels, all_preds, average='weighted')

    target_names = ['0', '1', '2']
    class_report = classification_report(all_labels, all_preds, target_names=target_names, digits=4)
    conf_matrix = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2])

    print(f"Test Accuracy: {accuracy:.3f}")
    print(f"Test Weighted F1 Score: {f1:.3f}")
    print("---------------------------------------------------------------------")
    print(f"Test Data:\n{class_report}")
    print(f"COnfusion Matrix:\n{conf_matrix}")
    
    return f1, class_report, conf_matrix

model = Model(dropout=0.2)
learning_rate = 5e-5
epochs = 10

history, best_model = train(model, df_train, learning_rate=learning_rate, epochs=epochs, batch_size=32)

with open("history_train_bertcnn_final.pkl", "wb") as f:
    pickle.dump(history, f)

torch.save(best_model.state_dict(), "best_model_bertcnn_final.pt")
device = torch.device("cuda")


# # Random Search
# learning_rates = [1e-4, 1e-5, 3e-5, 5e-5, 5e-6]
# dropouts = [0.1, 0.2, 0.3, 0.4, 0.5]
# batch_sizes = [16,32, 64]
# epochs_list = [5]
# num_folds_list = [2]
# num_trials = 30

# trial_results = []
# best_score = 0
# best_params = None
# best_model_overall = None

# for trial in range(num_trials):
#     lr = random.choice(learning_rates)
#     dropout = random.choice(dropouts)
#     batch_size = random.choice(batch_sizes)
#     epochs = random.choice(epochs_list)
#     num_folds = random.choice(num_folds_list)

#     print(f"\n==== Trial {trial+1}/{num_trials} ====")
#     print(f"Parameters: lr={lr}, dropout={dropout}, batch_size={batch_size}, epochs={epochs}, num_folds={num_folds}")

#     model = Model(dropout=dropout) 

#     history, best_trial_model, best_val_f1 = train(model, df_train, learning_rate=lr, epochs=epochs,
#                                      batch_size=batch_size, num_folds=num_folds)

#     trial_result = {
#         'trial': trial + 1,
#         'learning_rate': lr,
#         'dropout': dropout,
#         'batch_size': batch_size,
#         'epochs': epochs,
#         'num_folds': num_folds,
#         'avg_val_f1': best_val_f1,
#     }
#     trial_results.append(trial_result)

# print("\n========== Random Search Results ==========")
# print("Best Hyperparameters:", best_params)

# df_results = pd.DataFrame(trial_results)
# df_results.to_csv("bertcnn_trial_results.csv", index=False)